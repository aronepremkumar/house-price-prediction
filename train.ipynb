{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: joblib in ./env/lib/python3.12/site-packages (1.5.3)\n",
      "Requirement already satisfied: xgboost in ./env/lib/python3.12/site-packages (3.1.3)\n",
      "Requirement already satisfied: scikit-learn in ./env/lib/python3.12/site-packages (1.8.0)\n",
      "Requirement already satisfied: numpy in ./env/lib/python3.12/site-packages (from xgboost) (2.2.6)\n",
      "Requirement already satisfied: scipy in ./env/lib/python3.12/site-packages (from xgboost) (1.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in ./env/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install joblib xgboost scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in ./env/lib/python3.12/site-packages (0.24.1)\n",
      "Requirement already satisfied: numpy in ./env/lib/python3.12/site-packages (from torchvision) (2.2.6)\n",
      "Requirement already satisfied: torch==2.9.1 in ./env/lib/python3.12/site-packages (from torchvision) (2.9.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./env/lib/python3.12/site-packages (from torchvision) (12.1.0)\n",
      "Requirement already satisfied: filelock in ./env/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./env/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (4.15.0)\n",
      "Requirement already satisfied: setuptools in ./env/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./env/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./env/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in ./env/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in ./env/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (2026.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./env/lib/python3.12/site-packages (from sympy>=1.13.3->torch==2.9.1->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./env/lib/python3.12/site-packages (from jinja2->torch==2.9.1->torchvision) (3.0.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import joblib\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# 1. Load Data\n",
    "df = pd.read_csv('data/dataset.csv')\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# 2. Split Data - CRITICAL: Reset index to keep tabular and image paths aligned\n",
    "df_train_full, df_test = train_test_split(df, test_size=0.2, random_state=1)\n",
    "df_train, df_val = train_test_split(df_train_full, test_size=0.25, random_state=1)\n",
    "\n",
    "def prepare_tabular(data_frame, vectorizer=None, fit=False):\n",
    "    features = ['longitude', 'latitude', 'housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income', 'bedrooms', 'sq_ft']\n",
    "    dicts = data_frame[features].to_dict('records')\n",
    "    if fit:\n",
    "        return vectorizer.fit_transform(dicts), data_frame['median_house_value']\n",
    "    return vectorizer.transform(dicts), data_frame['median_house_value']\n",
    "\n",
    "dv = DictVectorizer(sparse=False) # Sparse=False for easier concat later\n",
    "X_train, y_train = prepare_tabular(df_train, dv, fit=True)\n",
    "X_val, y_val = prepare_tabular(df_val, dv)\n",
    "X_test, y_test = prepare_tabular(df_test, dv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Dataset & CNN Setup\n",
    "class HouseDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.paths = dataframe['image_path'].values\n",
    "        self.labels = dataframe['median_house_value'].values\n",
    "        self.transform = transform\n",
    "    def __len__(self): return len(self.paths)\n",
    "    def __getitem__(self, idx):\n",
    "        # Use path directly since data_prep.py saved full paths\n",
    "        try:\n",
    "            img = Image.open(self.paths[idx]).convert('RGB')\n",
    "        except:\n",
    "            img = Image.new('RGB', (200, 200), color='black') # Fallback for missing files\n",
    "        if self.transform: img = self.transform(img)\n",
    "        return img, torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)), # Reduced size for faster training on CPU\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_loader = DataLoader(HouseDataset(df_train, transform), batch_size=32, shuffle=False) # Shuffle=False to align with X_train\n",
    "val_loader = DataLoader(HouseDataset(df_val, transform), batch_size=32, shuffle=False)\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_stack = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2), # 128 -> 64\n",
    "            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2), # 64 -> 32\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        self.fc = nn.Linear(32 * 32 * 32, 64) # Adjust for 128x128 input\n",
    "        self.out = nn.Linear(64, 1)\n",
    "    def forward(self, x):\n",
    "        x = self.fc(self.conv_stack(x))\n",
    "        return self.out(torch.relu(x))\n",
    "    def extract(self, x):\n",
    "        return self.fc(self.conv_stack(x))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "cnn = CNN().to(device)\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting CNN features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arone/Documents/Projects/house-price-prediction/env/lib/python3.12/site-packages/PIL/Image.py:3451: DecompressionBombWarning: Image size (90344064 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fused RMSE: 50927.77\n"
     ]
    }
   ],
   "source": [
    "# 4. Fusion Feature Extraction\n",
    "def get_cnn_features(model, loader):\n",
    "    model.eval()\n",
    "    feats = []\n",
    "    with torch.no_grad():\n",
    "        for imgs, _ in loader:\n",
    "            feats.append(model.extract(imgs.to(device)).cpu().numpy())\n",
    "    return np.vstack(feats)\n",
    "\n",
    "print(\"Extracting CNN features...\")\n",
    "train_img_feats = get_cnn_features(cnn, train_loader)\n",
    "val_img_feats = get_cnn_features(cnn, val_loader)\n",
    "\n",
    "# 5. Combined XGBoost (Fusion)\n",
    "X_train_fused = np.hstack([X_train, train_img_feats])\n",
    "X_val_fused = np.hstack([X_val, val_img_feats])\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train_fused, label=y_train)\n",
    "dval = xgb.DMatrix(X_val_fused, label=y_val)\n",
    "\n",
    "params = {'objective': 'reg:squarederror', 'max_depth': 6, 'eta': 0.1}\n",
    "fused_model = xgb.train(params, dtrain, num_boost_round=50)\n",
    "\n",
    "y_pred = fused_model.predict(dval)\n",
    "print(f'Fused RMSE: {np.sqrt(mean_squared_error(y_val, y_pred)):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
